{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Extracting from cs101-pdfs/C++.pdf (846 pages) | Pages 22 to 803\n",
      "‚úÖ Extracted 965353 characters from cs101-pdfs/C++.pdf\n",
      "\n",
      "üìå Extracting from cs101-pdfs/The_C++_Programming_Language_4th_Edition_Bjarne_Stroustrup.pdf (1366 pages) | Pages 23 to 1278\n",
      "‚úÖ Extracted 2509426 characters from cs101-pdfs/The_C++_Programming_Language_4th_Edition_Bjarne_Stroustrup.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, start_page, end_page):\n",
    "    \"\"\"Extracts text from a given PDF file within a specific page range.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)  # Count total pages\n",
    "    print(f\"üìå Extracting from {pdf_path} ({total_pages} pages) | Pages {start_page} to {end_page}\")\n",
    "\n",
    "    full_text = []\n",
    "    for page_num in range(start_page - 1, min(end_page, total_pages)):  # Adjusting for 0-based index\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\")\n",
    "        if text.strip():  # Only count non-empty pages\n",
    "            full_text.append(text)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Empty text extracted from page {page_num + 1}\")\n",
    "\n",
    "    extracted_text = \"\\n\".join(full_text)\n",
    "    print(f\"‚úÖ Extracted {len(extracted_text)} characters from {pdf_path}\\n\")\n",
    "    return extracted_text\n",
    "\n",
    "# Define PDFs and their required page ranges\n",
    "pdf_folder = \"cs101-pdfs/\"\n",
    "documents = {}\n",
    "\n",
    "pdf_configs = {\n",
    "    \"C++.pdf\": (22, 803),\n",
    "    \"The_C++_Programming_Language_4th_Edition_Bjarne_Stroustrup.pdf\": (23, 1278)\n",
    "}\n",
    "\n",
    "for pdf_file, (start_page, end_page) in pdf_configs.items():\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    if os.path.exists(pdf_path):\n",
    "        text = extract_text_from_pdf(pdf_path, start_page, end_page)\n",
    "        documents[pdf_file] = text\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {pdf_path}\")\n",
    "\n",
    "# You can now process `documents` as needed, e.g., save to text files or store in a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Clean and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå After Cleaning: 1579 characters from The_C++_Programming_Language_4th_Edition_Bjarne_Stroustrup.pdf\n",
      "üìå After Cleaning: 6221 characters from C++.pdf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes everything before the first chapter title and cleans the text.\"\"\"\n",
    "    # Find the first occurrence of a chapter heading\n",
    "    match = re.search(r'(?i)(chapter\\s*\\d+|1[. ]\\s+|CHAPTER ONE|CHAPTER 1|1[.]\\d+)', text)\n",
    "\n",
    "    if match:\n",
    "        start_index = match.start()  # Get where the first chapter starts\n",
    "        text = text[start_index:]  # Keep only content after the match\n",
    "\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Apply new cleaning function\n",
    "for pdf_name, content in documents.items():\n",
    "    documents[pdf_name] = clean_text(content)\n",
    "\n",
    "# Debug: Check cleaned text size\n",
    "for pdf_name, content in documents.items():\n",
    "    print(f\"üìå After Cleaning: {len(content)} characters from {pdf_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m PINECONE_INDEX_NAME \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPINECONE_INDEX_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize Pinecone\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPINECONE_API_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPINECONE_ENV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m index \u001b[38;5;241m=\u001b[39m pinecone\u001b[38;5;241m.\u001b[39mIndex(PINECONE_INDEX_NAME)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load extracted embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/RAG-Dataset/.venv/lib/python3.12/site-packages/pinecone/deprecation_warnings.py:39\u001b[0m, in \u001b[0;36minit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    import os\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m    from pinecone import Pinecone, ServerlessSpec\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m        )\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124minit is no longer a top-level attribute of the pinecone package.\u001b[39m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mPlease create an instance of the Pinecone class instead.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index', \n            dimension=1536, \n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken  # For accurate token-based chunking\n",
    "\n",
    "# Load OpenAI's tokenizer for GPT-4 to ensure we stay within limits\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 tokenizer\n",
    "\n",
    "# Pinecone max vector size (keeping it safe under 4MB)\n",
    "MAX_TOKENS_PER_CHUNK = 3000  # Adjust based on your model's token limit\n",
    "\n",
    "def chunk_text_safely(text, chunk_size=MAX_TOKENS_PER_CHUNK, overlap=200):\n",
    "    \"\"\"Splits text into smaller chunks while keeping token count safe for Pinecone.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,  \n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \".\"],  # Keeps natural breaks\n",
    "        length_function=lambda x: len(tokenizer.encode(x))  # Count tokens instead of characters\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Apply chunking to all cleaned documents\n",
    "chunked_data = []\n",
    "for pdf_name, content in documents.items():\n",
    "    chunks = chunk_text_safely(content)\n",
    "    print(f\"‚úÖ {len(chunks)} chunks created for {pdf_name}\")  # Debugging output\n",
    "    for chunk in chunks:\n",
    "        chunked_data.append({\n",
    "            \"text\": chunk,\n",
    "            \"source\": pdf_name  # Metadata for retrieval\n",
    "        })\n",
    "\n",
    "print(f\"\\nüìå Total Chunks Created: {len(chunked_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:   8%|‚ñä         | 1/12 [00:11<02:10, 11.85s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  17%|‚ñà‚ñã        | 2/12 [00:21<01:43, 10.32s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  25%|‚ñà‚ñà‚ñå       | 3/12 [00:30<01:28,  9.89s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  33%|‚ñà‚ñà‚ñà‚ñé      | 4/12 [00:33<00:58,  7.35s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Request failed: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\n",
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 [00:39<00:47,  6.83s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Request failed: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\n",
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 6/12 [00:44<00:36,  6.03s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Request failed: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\n",
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7/12 [00:56<00:40,  8.20s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 [01:03<00:31,  7.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 9/12 [01:14<00:26,  8.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 [01:31<00:22, 11.06s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 11/12 [01:40<00:10, 10.44s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:42<00:00,  8.11s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Progress saved to embeddings_progress.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:44<00:00,  8.70s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Hugging Face embeddings generated for all chunks!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load Hugging Face API key from environment\n",
    "hf_api_key = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Define Hugging Face Inference API URL (Correct Endpoint for Feature Extraction)\n",
    "API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "headers = {\"Authorization\": f\"Bearer {hf_api_key}\"}\n",
    "\n",
    "def get_hf_embedding(texts):\n",
    "    \"\"\"Fetches embeddings for a batch of text chunks from Hugging Face Inference API.\"\"\"\n",
    "    payload = {\"inputs\": texts}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        embeddings = response.json()\n",
    "        \n",
    "        if isinstance(embeddings, list) and all(isinstance(i, list) for i in embeddings):\n",
    "            return embeddings  # Return list of embeddings\n",
    "        else:\n",
    "            print(\"‚ùå API returned unexpected format:\", embeddings)\n",
    "            return None\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_embeddings(chunked_data, batch_size=30):\n",
    "    \"\"\"Processes embeddings in batches synchronously with a progress bar.\"\"\"\n",
    "    total_batches = len(chunked_data) // batch_size + (1 if len(chunked_data) % batch_size != 0 else 0)\n",
    "    \n",
    "    with tqdm(total=total_batches, desc=\"Processing Embeddings\", unit=\"batch\") as pbar:\n",
    "        for i in range(0, len(chunked_data), batch_size):\n",
    "            batch = chunked_data[i:i+batch_size]\n",
    "            texts = [data[\"text\"] for data in batch]\n",
    "            \n",
    "            embeddings = get_hf_embedding(texts)\n",
    "            \n",
    "            if embeddings:\n",
    "                for idx, embedding in enumerate(embeddings):\n",
    "                    chunked_data[i + idx][\"embedding\"] = embedding\n",
    "            \n",
    "            pbar.update(1)  # Update progress bar\n",
    "            \n",
    "            # Save progress every batch\n",
    "            save_progress(chunked_data)\n",
    "            \n",
    "            # Respect Hugging Face rate limits (adjust if needed)\n",
    "            time.sleep(1.5)  # Small delay to prevent rate limit issues\n",
    "\n",
    "    print(\"\\n‚úÖ Hugging Face embeddings generated for all chunks!\")\n",
    "\n",
    "def save_progress(chunked_data):\n",
    "    \"\"\"Saves current progress to a JSON file.\"\"\"\n",
    "    with open(\"embeddings_progress.json\", \"w\") as f:\n",
    "        json.dump(chunked_data, f, indent=4)\n",
    "    print(\"üíæ Progress saved to embeddings_progress.json\")\n",
    "\n",
    "# Run the embedding extraction\n",
    "if __name__ == \"__main__\":\n",
    "    process_embeddings(chunked_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Store to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Pinecone API key from environment variables\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"PINECONE_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create and Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index parameters\n",
    "index_name = \"cs101-rag\"\n",
    "dimension = 384  # Dimensionality of MiniLM embeddings\n",
    "\n",
    "# Check if the index already exists\n",
    "if not pc.has_index(index_name):\n",
    "    # Create a new index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=\"cosine\",  # Using cosine similarity\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Upsert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting into Pinecone: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:07<00:00,  1.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully upserted 241 vectors into Pinecone!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "PROGRESS_FILE = \"embeddings_progress.json\"\n",
    "\n",
    "def load_embeddings():\n",
    "    \"\"\"Loads saved embeddings from progress file.\"\"\"\n",
    "    if not os.path.exists(PROGRESS_FILE):\n",
    "        raise FileNotFoundError(f\"‚ùå {PROGRESS_FILE} not found. Run embedding extraction first.\")\n",
    "    \n",
    "    with open(PROGRESS_FILE, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def upsert_embeddings(batch_size=100):\n",
    "    \"\"\"Upserts embeddings into Pinecone in batches.\"\"\"\n",
    "    embeddings_data = load_embeddings()\n",
    "    upserted_count = 0\n",
    "\n",
    "    total_batches = len(embeddings_data) // batch_size + (1 if len(embeddings_data) % batch_size != 0 else 0)\n",
    "\n",
    "    with tqdm(total=total_batches, desc=\"Upserting into Pinecone\", unit=\"batch\") as pbar:\n",
    "        for i in range(0, len(embeddings_data), batch_size):\n",
    "            batch = embeddings_data[i:i + batch_size]\n",
    "\n",
    "            # Filter out entries without embeddings\n",
    "            valid_vectors = [\n",
    "                {\n",
    "                    \"id\": str(hash(data[\"text\"])),  # Unique ID for each text\n",
    "                    \"values\": data[\"embedding\"],  # Embedding vector\n",
    "                    \"metadata\": {\n",
    "                        \"source\": data.get(\"source\", \"unknown\"),  # Default to \"unknown\" if missing\n",
    "                        \"text\": data[\"text\"]\n",
    "                    }\n",
    "                }\n",
    "                for data in batch if \"embedding\" in data and isinstance(data[\"embedding\"], list)\n",
    "            ]\n",
    "\n",
    "            if not valid_vectors:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Upsert with retries\n",
    "            success = upsert_with_retries(valid_vectors)\n",
    "            if success:\n",
    "                upserted_count += len(valid_vectors)\n",
    "\n",
    "            pbar.update(1)  # Update progress bar\n",
    "            time.sleep(0.5)  # Prevent hitting rate limits\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully upserted {upserted_count} vectors into Pinecone!\")\n",
    "\n",
    "def upsert_with_retries(vectors, retries=3):\n",
    "    \"\"\"Attempts to upsert into Pinecone with retry logic.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            index.upsert(vectors=vectors)\n",
    "            return True  # Success\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Upsert attempt {attempt + 1}/{retries} failed: {e}\")\n",
    "            time.sleep(2)  # Small delay before retrying\n",
    "    \n",
    "    print(\"‚ùå Upsert failed after all retries.\")\n",
    "    return False\n",
    "\n",
    "# Run upsertion\n",
    "if __name__ == \"__main__\":\n",
    "    upsert_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of vectors in the index: 241\n",
      "\n",
      "‚úÖ Sample Vector Retrieved:\n",
      "ID: 4668887154771494313\n",
      "Values: [-0.0561828651, 0.039083492, 0.0347332954, -0.0338819, -0.0035442498, -0.0463199802, -0.0232972614, 0.0666727796, -0.0610934347, -0.0591209084, 0.040451929, 0.0116449352, -0.0427011177, 0.0234538037, -0.0651997477, -0.023923818, -0.055158861, 0.0596844964, 0.0120901512, -0.0279157441, 0.0945512578, 0.0292616468, -0.0812079385, 0.0168211386, 0.0416575447, -0.000626907102, -0.0589060634, 0.00426082546, 0.0620645173, -0.0142065203, 0.0886903778, 0.00592241203, 0.096780315, 0.0287935808, -0.00703592459, 0.0132592367, -0.0142796757, -0.0604704805, -0.0362280495, 0.0713307485, 0.048421666, 0.0144479871, -0.00939351134, 0.0382677913, 0.0035734491, 0.0347920842, -0.0164370481, -0.0183145572, -0.0890184864, 0.0489395373, -0.0500382781, 0.0920795947, -0.0782784373, 0.0781494603, 0.071398966, -0.0265639517, -0.0230601523, -0.0621126033, 0.0348485708, 0.0069211293, -0.00854103081, -0.033824306, 0.0290782489, -0.107157916, 0.0495366305, -0.0245755184, 0.0197016262, -0.00699572312, -0.00696769496, 0.0970686898, -0.0400879681, 0.0239396989, -0.0418552272, 0.060616266, -0.036102064, 0.00593494764, 0.0758385286, -0.00864192657, -0.0640692189, -0.0550016351, -0.0976449773, 0.024724979, 0.0328769535, -0.03501039, -0.0474671759, 0.00871737953, -0.0353161693, 0.0073488215, 0.0986634865, 0.0102014448, 0.0274878647, -0.0176924095, -0.0877468958, 0.0313065648, 0.056911815, -0.0255202092, 0.0237672888, -0.00781699363, 0.0459288321, 0.0210298114, 0.0346505381, 0.0175242051, 0.0447964221, -0.00626482, -0.0429576673, -0.00323634897, -0.0322953127, -0.041034013, -0.0939767361, -0.00434479211, 0.0265672281, -0.000530766032, 0.00914817, -0.0282499, -0.0073634, -0.0394867733, -0.00365951797, -0.00149122055, 0.180957153, 0.0110683627, 0.00645801099, -0.00829309318, -0.0147252111, 0.055727575, 0.0247908812, 0.0259969849, 0.000143702026, 2.04645522e-33, -0.0242954586, 0.0550853088, 0.0118609499, 0.0171469357, -0.0529196039, -0.0154355532, 0.0108191967, -0.0403565727, -0.0777581558, 0.0444771461, -0.0496785156, 0.0316320285, 0.0762932226, 0.0909661129, 0.0443297662, -0.0795194209, 0.148927659, -0.00539860735, -0.0457961485, -0.0872357115, 0.0760632679, 0.00426565157, -0.0292874817, 0.0392012037, -0.00986705814, -0.0570703149, -0.0439020023, -0.0801391676, -0.0983837619, 0.0124307582, -0.0922280326, 0.0122928182, 0.00088036, -0.0628814921, 0.163640946, 0.0366156846, -0.000520172063, -0.00831343327, 0.0184099935, -0.0700242221, 0.0518442132, 0.020664094, 0.0255017672, -0.00777653512, -0.0706758127, -0.0307831764, -0.0192098338, 0.0686106682, -0.0152767049, 0.0147352275, 0.0270977356, 0.0144394878, 0.0562792942, -0.0325104222, 0.0458284691, -0.0137387915, 0.0477887914, 0.0418518744, 0.0296818651, 0.097601451, -0.0479000099, 0.0714380145, 0.0155554172, 0.0768459216, -0.0197077375, -0.0143443989, -0.0895427242, 0.0152403051, 0.0354389437, 0.0237868447, -0.0690648332, 0.0287548676, -0.0115423827, -0.00615320401, -0.04069883, -0.0136079881, 0.000481603289, -0.0721706375, -0.0308911577, -0.105361283, -0.0851258487, 0.120297365, 0.0437104888, -0.118506275, -0.0130615747, -0.0948826894, -0.0149354488, -0.0360570699, -0.0439026132, -0.0673350915, -0.0243255366, -0.0541553609, 0.0240204241, -0.0284613669, -0.0474585034, -2.99329057e-33, 0.00977389701, 0.010109866, 0.0744834095, 0.0255563501, -0.0851380527, -0.0814230815, 0.0795109197, 0.00546582183, -0.018207645, 0.00937228, -0.0900238752, 0.0460112765, 0.0621149614, -0.0391308963, 0.0810009, -0.0105135841, -0.0561787672, -0.0518170446, 0.0262704771, 0.000989946187, -0.0331988297, -0.0104261925, 0.0446152911, 0.00978818256, -0.042236425, 0.0329340324, 0.0806683153, 0.0087171318, -0.0265696049, -0.0826233, -0.0127487192, -0.111457914, 0.00765806204, -0.00373650319, 0.0442980789, 0.0411555655, -0.0268160366, 0.0505771413, 0.0490425192, -0.0410853662, 0.00119319581, 0.0127055012, 0.10714101, 0.0569624566, -0.00925439782, 0.0487772748, 0.0635898486, 0.0258706287, -0.0967924818, -0.0403603874, -0.00557991536, 0.0494724, 0.0212875418, 0.0204554833, -0.0391255617, -0.0226741359, -0.106457353, 0.00668716896, -0.0273220576, -0.0409112796, -0.0246074181, -0.0752671361, 0.0670068637, -0.00253738393, 0.079836823, -0.0457205288, -0.0739242062, -0.125180364, -0.0193939526, -0.0131405191, -0.00498264795, 0.0534397028, 0.026785899, -0.0191155337, -0.145663112, -0.0714085624, 0.0255920477, 0.035997875, -0.0381449759, 0.0209061727, -0.0814135224, 0.0272025075, 0.015360645, 0.0551813804, -0.0455803461, 0.00194460771, 0.0784392431, 0.0511787087, 0.0218074713, -0.0658519417, 0.013670587, 0.0147368694, 0.00120006956, 0.0626257062, 0.0618576556, -4.45788118e-08, -0.0514581725, -0.036048647, -0.0414387956, 0.0271913707, 0.0651428103, -0.0171027649, 0.0521944873, 0.00608394528, 0.0179046709, 0.0379451029, 0.0839926302, 0.0358030908, 0.000412452064, -0.0295799635, 0.0448751636, -0.000707145664, 0.0077487058, -0.0271324534, -0.0297857132, 0.0893737525, -0.0486412384, 0.0137819508, -0.013985185, 0.0123405261, -0.0263397619, -0.066938065, -0.0180399083, 0.00409301603, 0.105002761, -0.0451471917, 0.05919585, 0.0751659498, 0.109621421, 0.105162323, 0.00926880352, 0.00290709, -0.0539941527, 0.0795341432, 0.0210166723, -0.0169633869, 0.00975463446, 0.00256349379, 0.0177477095, 0.0218473785, -0.0509387627, 0.0012064887, -0.120450564, 0.0491060205, -0.0455852486, 0.0155645637, -0.0483804047, 0.0290157422, -0.0486123487, 0.0855511203, 0.0130575038, -0.015540354, -0.0492792875, -0.062659584, 0.0335839875, 0.0363986865, -0.0098267775, -0.0361549966, -0.0308315568, 0.00979411788]\n",
      "Metadata: {'source': 'The_C++_Programming_Language_4th_Edition_Bjarne_Stroustrup.pdf', 'text': 'Section 29.4.1\\nslice()\\n841\\n29.4.1 slice()\\nA simple slice as used for slice subscripting describes a mapping from an integer (subscript) to an\\nelement location (index) in terms of three values:\\nstruct slice {\\nslice() :start(‚àí1), length(‚àí1), stride(1) { }\\nexplicit slice(size_t s) :start(s), length(‚àí1), stride(1) { }\\nslice(size_t s, size_t l, size_t n = 1) :start(s), length(l), stride(n) { }\\nsize_t operator()(siz e_t i) const { return start+i‚àóstride; }\\nstatic slice all;\\nsize_t star t;\\n// Ô¨Årst index\\nsize_t length; \\n// number of indices included (can be used for range checking)\\nsize_t stride; \\n// distance between elements in sequence\\n};\\nThere is a standard-library version of slice; see ¬ß40.5.4 for a more thorough discussion. This ver-\\nsion provides notational convenience (e.g., the default values provided by the constructors).\\n29.4.2 Matrix Slices\\nA Matrix_slice is the part of the Matrix implementation that maps a set of subscripts to the location\\nof an element. It uses the idea of generalized slices (¬ß40.5.6):\\ntemplate<size_t N>\\nstruct Matrix_slice {\\nMatrix_slice() = default;\\n// an empty matrix: no elements\\nMatrix_slice(size_t s, initializer_list<siz e_t> exts); // extents\\nMatrix_slice(size_t s, initializer_list<siz e_t> exts, initializer_list<siz e_t> strs);// extents and strides\\ntemplate<typename... Dims> \\n// N extents\\nMatrix_slice(Dims... dims);\\ntemplate<typename... Dims,\\ntypename = Enable_if<All(Conver tible<Dims,siz e_t>()...)>>\\nsize_t operator()(Dims... dims) const;\\n// calculate index from a set of subscripts\\nsize_t siz e;\\n// total number of elements\\nsize_t star t;\\n// star ting offset\\narray<siz e_t,N> extents; \\n// number of elements in each dimension\\narray<siz e_t,N> strides; \\n// offsets between elements in each dimension\\n};\\nIn other words, a Matrix_slice describes what is considered rows and columns in a region of mem-\\nory. In the usual C/C++ row-major layout of a matrix, the elements of rows are contiguous, and the\\nelements of a column are separated by a Ô¨Åxed number of elements (a stride). A Matrix_slice is a\\nfunction object, and its operator()() does a stride calculation (¬ß40.5.6):\\n\\n842 \\nA Matrix Design\\nChapter 29\\ntemplate<size_t N>\\ntemplate<typename... Dims>\\nsize_t Matrix_slice<N>::operator()(Dims... dims) const\\n{\\nstatic_assert(sizeof...(Dims) == N, \"\");\\nsize_t args[N] { size_t(dims)... };\\n// Copy arguments into an array\\nreturn inner_product(args,args+N,strides.begin(),siz e_t(0));\\n}\\nSubscripting must be efÔ¨Åcient. This is a simpliÔ¨Åed algorithm that needs to be optimized. If nothing\\nelse, specialization can be used to eliminate the simplifying copy of subscripts out of the variadic\\ntemplate‚Äôs parameter pack. For example:\\ntemplate<>\\nstruct Matrix_slice<1> {\\n// ...\\nsize_t operator()(siz e_t i) const\\n{\\nreturn i;\\n}\\n}\\ntemplate<>\\nstruct Matrix_slice<2> {\\n// ...\\nsize_t operator()(siz e_t i, size_t j) const\\n{\\nreturn i‚àóstides[0]+j;\\n}\\n}\\nThe Matrix_slice is fundamental for deÔ¨Åning the shape of a Matrix (its extents) and for implementing\\nN-dimensional subscripting. However, it is also useful for deÔ¨Åning submatrices.\\n29.4.3 Matrix_ref\\nA Matrix_ref is basically a clone of the Matrix class used to represent sub-Matrixes. However, a\\nMatrix_ref does not own its elements. It is constructed from a Matrix_slice and a pointer to elements:\\ntemplate<typename T, siz e_t N>\\nclass Matrix_ref {\\npublic:\\nMatrix_ref(const Matrix_slice<N>& s, T‚àóp) :desc{s}, ptr{p} {}\\n// ... mostly like Matr ix ...\\n\\nSection 29.4.3\\nMatrix_ref\\n843\\nprivate:\\nMatrix_slice<N> desc;\\n// the shape of the matrix\\nT‚àóptr; \\n// the Ô¨Årst element in the matrix\\n};\\nA Matrix_ref simply points to the elements of ‚Äò‚Äòits‚Äô‚Äô Matrix. Obviously, a Matrix_ref should not out-\\nlive its Matrix. For example:\\nMatrix_ref<double ,1> user()\\n{\\nMatrix<double ,2> m = {{1,2}, {3,4}, {5,6}};\\nreturn m.row(1);\\n}\\nauto mr = user();\\n// trouble\\nThe great similarity between Matrix and Matrix_ref leads to duplication. If that becomes a bother,\\nwe can derive both from a common base:\\ntemplate<typename T, siz e_t N>\\nclass Matrix_base {\\n// ... common stuff ...\\n};\\ntemplate<typename T, siz e_t N>\\nclass Matrix : public Matrix_base<T,N> {\\n// ... special to Matrix ...\\nprivate:\\nMatrix_slice<N> desc;\\n// the shape of the matrix\\nvector<T> elements;\\n};\\ntemplate<typename T, siz e_t N>\\nclass Matrix_ref : public Matrix_base<T,N> {\\n// ... special to Matrix_ref ...\\nprivate:\\nMatrix_slice<N> desc;\\n// the shape of the matrix\\nT‚àóptr;\\n};\\n29.4.4 Matrix List Initialization\\nThe Matrix constructor that constructs from an initializer_list takes as its argument type the alias\\nMatrix_initializer:\\ntemplate<typename T, siz e_t N>\\nusing Matrix_initializer = typename Matrix_impl::Matrix_init<T, N>::type;\\nMatrix_init describes the structure of a nested initializer_list.\\n\\n844 \\nA Matrix Design\\nChapter 29\\nMatrix_init<T,N> simply has Matrix_init<T,N‚àí1> as its member type:\\ntemplate<typename T, siz e_t N>\\nstruct Matrix_init {\\nusing type = initializer_list<typename Matrix_init<T,N‚àí1>::type>;\\n};\\nThe N==1 is special. That is where we get to the (most deeply nested) initializer_list<T>:\\ntemplate<typename T>\\nstruct Matrix_init<T,1> {\\nusing type = initializer_list<T>;\\n};\\nTo avoid surprises, we deÔ¨Åne N=0 to be an error:\\ntemplate<typename T>\\nstruct Matrix_init<T,0>; // undeÔ¨Åned on purpose\\nWe can now complete the Matrix constructor that takes a Matrix_initializer:\\ntemplate<typename T, siz e_t N>\\nMatrix<T, N>::Matrix(Matrix_initializ er<T,N> init)\\n{\\nMatrix_impl::derive_extents(init,desc.extents); \\n// deduce extents from initializer list (¬ß29.4.4)\\nelems.reserve(desc.size);\\n// make room for slices\\nMatrix_impl::insert_Ô¨Çat(init,elems);\\n// initialize from initializer list (¬ß29.4.4)\\nassert(elems.size() == desc.size);\\n}\\nTo do so, we need two operations that recurse down a tree of initializer_lists for a Matrix<T,N>:\\n‚Ä¢\\nderive_extents() determines the shape of the Matrix:\\n‚Ä¢\\nChecks that the tree really is N deep\\n‚Ä¢\\nChecks that each row (sub-initialize_list) has the same number of elements\\n‚Ä¢\\nSets the extent of each row\\n‚Ä¢\\ninsert_Ô¨Çat() copies the elements of the tree of initializer_list<T>s into the elems of a Matrix.\\nThe derived_extents() called from a Matrix constructor to initialize its desc looks like this:\\ntemplate<size_t N, typename List>\\narray<siz e_t, N> derive_extents(const List& list)\\n{\\narray<siz e_t,N> a;\\nauto f = a.begin();\\nadd_extents<N>(f,list); \\n// put extents from list into f[]\\nreturn a;\\n}\\nYou giv e it an initializer_list and it returns an array of extents.\\nThe recursion is done from N to the Ô¨Ånal 1 where the initializer_list is an initializer_list<T>.\\n\\nSection 29.4.4\\nMatrix List Initialization\\n845\\ntemplate<size_t N, typename I, typename List>\\nEnable_if<(N>1),void> add_extents(I& Ô¨Årst, const List& list)\\n{\\nassert(check_non_jagged(list));\\n‚àóÔ¨Årst = list.size();\\nadd_extents<N‚àí1>(++Ô¨Årst,‚àólist.begin());\\n}\\ntemplate<size_t N, typename I, typename List>\\nEnable_if<(N==1),void> add_extents(I& Ô¨Årst, const List& list)\\n{\\n‚àóÔ¨Årst++ = list.size(); \\n// we reached the deepest nesting\\n}\\nThe check_non_jagged() function checks that all rows have the same number of elements:\\ntemplate<typename List>\\nbool check_non_jagged(const List& list)\\n{\\nauto i = list.begin();\\nfor (auto j = i+1; j!=list.end(); ++j)\\nif (i‚àí>size()!=j‚àí>siz e())\\nreturn false;\\nreturn true;\\n}\\nWe need insert_Ô¨Çat() to take a possibly nested initializer list and present its elements to Matrix<T> as\\na vector<T>. It takes the initializer_list given to a Matrix as the Matrix_initializer and provides the ele-\\nments as the target:\\ntemplate<typename T, typename Vec>\\nvoid insert_Ô¨Çat(initializ er_list<T> list, Vec& vec)\\n{\\nadd_list(list.begin(),list.end(),vec);\\n}\\nUnfortunately, we can‚Äôt rely on the elements being allocated contiguously in memory, so we need\\nto build the vector through a set of recursive calls. If we have a list of initializer_lists, we recurse\\nthrough each:\\ntemplate<typename T, typename Vec> \\n// nested initializer_lists\\nvoid add_list(const initializer_list<T>‚àóÔ¨Årst, const initializer_list<T>‚àólast, Vec& vec)\\n{\\nfor (;Ô¨Årst!=last;++Ô¨Årst)\\nadd_list(Ô¨Årst‚àí>begin(),Ô¨Årst‚àí>end(),vec);\\n}\\nWhen we reach a list with non-initializer_list elements, we insert those elements into our vector:\\n\\n846 \\nA Matrix Design\\nChapter 29\\ntemplate<typename T, typename Vec>\\nvoid add_list(const T‚àóÔ¨Årst, const T‚àólast, Vec& vec)\\n{\\nvec.insert(vec.end(),Ô¨Årst,last);\\n}\\nI use vec.insert(vec.end(),Ô¨Årst,last) because there is no push_back() that takes a sequence argument.\\n29.4.5 Matrix Access\\nA Matrix provides access by row, column, slice (¬ß29.4.1), and element (¬ß29.4.3). A row() or col-\\numn() operation returns a Matrix_ref<T,N‚àí1>, the () subscript operation with integers returns a T&, and\\nthe () subscript operation with slices returns a Matrix<T,N>.\\nThe row of a Matrix<T,N> is a Matrix_ref<T,N‚àí1> as long as 1<N:\\ntemplate<typename T, siz e_t N>\\nMatrix_ref<T,N‚àí1> Matrix<T,N>::row(siz e_t n)\\n{\\nassert(n<rows());\\nMatrix_slice<N‚àí1> row;\\nMatrix_impl::slice_dim<0>(n,desc,row);\\nreturn {row,data()};\\n}\\nWe need specializations for N==1 and N==0:\\ntemplate<typename T>\\nT& Matrix<T,1>::row(siz e_t i)\\n{\\nreturn &elems[i];\\n}\\ntemplate<typename T>\\nT& Matrix<T,0>::row(siz e_t n) = delete;\\nSelecting a column() is essentially the same as selecting a row(). The difference is simply in the\\nconstruction of the Matrix_slice:\\ntemplate<typename T, siz e_t N>\\nMatrix_ref<T,N‚àí1> Matrix<T,N>::column(siz e_t n)\\n{\\nassert(n<cols());\\nMatrix_slice<N‚àí1> col;\\nMatrix_impl::slice_dim<1>(n,desc,col);\\nreturn {col,data()};\\n}\\nThe const versions are equivalent.\\nRequesting_element() and Requesting_slice() are concepts for a set of integers used for subscript-\\ning with a set of integers and subscripting by a slice, respectively (¬ß29.4.5). They check that a\\nsequence of access-function arguments are of suitable types for use as subscripts.'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Pinecone API key from environment variables\n",
    "pinecone_api = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api)\n",
    "\n",
    "# Connect to your index\n",
    "index = pc.Index('cs101-rag')\n",
    "\n",
    "def list_vector_count():\n",
    "    \"\"\"Lists the total number of vectors in the index.\"\"\"\n",
    "    stats = index.describe_index_stats()\n",
    "    total_vectors = stats['total_vector_count']\n",
    "    print(f\"Total number of vectors in the index: {total_vectors}\")\n",
    "\n",
    "def fetch_random_vector():\n",
    "    \"\"\"Fetches a random vector ID from Pinecone to verify data integrity.\"\"\"\n",
    "    # List all vector IDs (this may need to be adjusted for large datasets)\n",
    "    vector_ids = []\n",
    "    for ids in index.list():\n",
    "        vector_ids.extend(ids)\n",
    "    \n",
    "    if not vector_ids:\n",
    "        print(\"No vectors found in the index.\")\n",
    "        return\n",
    "\n",
    "    # Select a random vector ID\n",
    "    random_id = random.choice(vector_ids)\n",
    "    \n",
    "    # Fetch the vector data\n",
    "    vector_data = index.fetch(ids=[random_id])\n",
    "    \n",
    "    if vector_data and random_id in vector_data.vectors:\n",
    "        vector_info = vector_data.vectors[random_id]\n",
    "        print(\"\\n‚úÖ Sample Vector Retrieved:\")\n",
    "        print(f\"ID: {random_id}\")\n",
    "        print(f\"Values: {vector_info.values}\")\n",
    "        print(f\"Metadata: {vector_info.metadata}\")\n",
    "    else:\n",
    "        print(\"Vector not found.\")\n",
    "\n",
    "# Run verification\n",
    "list_vector_count()\n",
    "fetch_random_vector()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
